
---
title: "Exploratory analysis"
author: "jagdeep s sihota"
date: "July 24, 2015"
output: html_document
---


# Synnopsis
The goal of this report is to perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora and understand frequencies of words and word pairs. Around the world, people are spending an increasing amount of time on their mobile devices for email, social networking, banking and a whole range of other activities. But typing on mobile devices can be a serious pain.  We will work on understanding and building predictive text models like those used by SwiftKey. This report will start with the basics, analyzing a large corpus of text documents to discover the structure in the data and how words are put together. 

  
# Dataset
The data is from a corpus called HC Corpora (www.corpora.heliohost.org). See the readme file at http://www.corpora.heliohost.org/aboutcorpus.html for details on the corpora available. The files have been language filtered but may still contain some foreign text. The dataset can be downloaded from https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip.

The dataset consists of data collected from blogs posts, twitter tweets, and online news articles. The dataset from these data sources are saved in .txt format as:

* en_US.blogs.txt
* en_US.twitter.txt
* en_US.news.txt

The data sources come in 4 different languages:

* US English (en_US)
* Finnish (fi_FI)
* Russian (ru_RU)
* German (de_DE)



```{r, echo=FALSE,chunk1,cache=TRUE,message=F, warning=F}
setwd("~/git/DataScienceCapstone")
# download file from URL
if (!file.exists("Coursera-SwiftKey.zip")) {
  download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", destfile="Coursera-SwiftKey.zip",method = "curl")
}
# unzip file
if (!file.exists("final")) {
  unzip("Coursera-SwiftKey.zip")
}

setwd("~/git/DataScienceCapstone/final/en_US")
# Read files into variables
newsData <- readLines(file("en_US.news.txt"))
blogData <- readLines(file("en_US.blogs.txt"))
twitterData <- readLines(file("en_US.twitter.txt"))

library(plyr)
library(knitr)
## Create the list for summary
list_en<-list(newsData, blogData,twitterData)
names(list_en)<- c("en_US_news", "en_US_blogs","en_US_twitter")

## Define the function for Word-counting
WordCounting<-function(x) length(unlist(strsplit(x,split = " ")))
ldply(list_en,c("object.size","length","WordCounting"))->output
names(output) <- c("File Name","Size in bytes", "Line Counts", "Word Counts")
```

# Summarise the data
```{r kable, echo=FALSE, include = TRUE,message=F, warning=F}
## Output the summary:Object size,Lines,Words
library(knitr)
kable(output)
```
## sampling data and data cleanup
For  performance reasons, only the first 10000 lines are loaded for the purpose of this report and cleanup the data by removing following:  

* Whitespace
* Numbers
* Punctuation
* stopwords

# Exploratory Data Analysis

## Word Frequency
Frequencies of top 30 words in combined 30000 lines

```{r, echo=FALSE,chunk2,cache=TRUE,message=F, warning=F}
library(tm)
merged <- paste(newsData[1:10000], blogData[1:10000], twitterData[1:10000])
corpus <- VCorpus(VectorSource(merged))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords())
```

```{r , echo=FALSE,message=F, warning=F}
library(RWeka)
library(tm)
doc <- TermDocumentMatrix(corpus)
M1<-as.matrix(doc)
names <- rownames(M1)
freq <- rowSums(M1)
order<-order(freq,decreasing = T)
data1 <- data.frame(id = names,
                   news = M1[,1],
                   blogs = M1[,2],
                  twitter = M1[,3],
                  freq = freq)
data1 <- data1[order,] 
data1[1:30,]
```

## Word pairs 
```{r, echo=FALSE,chunk3,cache=TRUE,message=F, warning=F}
corpusDf <-data.frame(text=unlist(sapply(corpus, 
                  `[`, "content")), stringsAsFactors=F)

findNGrams <- function(corp, grams) {
  ngram <- NGramTokenizer(corp, Weka_control(min = grams, max = grams,
                      delimiters = " \\r\\n\\t.,;:\"()?!"))
  ngram2 <- data.frame(table(ngram))
  #pick only top 25
  ngram3 <- ngram2[order(ngram2$Freq,decreasing = TRUE),][1:100,]
  colnames(ngram3) <- c("String","Count")
  ngram3
}

TwoGrams <- findNGrams(corpusDf, 2)
ThreeGrams <- findNGrams(corpusDf, 3)
FourGrams <- findNGrams(corpusDf, 4)
```

```{r, echo=FALSE,message=F, warning=F}
require(RColorBrewer)
library(wordcloud)


par(mfrow = c(1, 4))
palette <- brewer.pal(8,"Dark2")
set.seed(123)
data1[1:100,]->plotwordcloud
pal <- brewer.pal(9,"BuGn")
    pal <- pal[-(1:4)]
wordcloud(plotwordcloud$id,freq=plotwordcloud$freq,random.order = F,color=pal)

wordcloud(TwoGrams[,1], TwoGrams[,2], min.freq =1, 
          random.order = F, ordered.colors = F, colors=palette)
text(x=0.5, y=0, "2-gram cloud")

wordcloud(ThreeGrams[,1], ThreeGrams[,2], min.freq =1, 
          random.order = F, ordered.colors = F, colors=palette)
text(x=0.5, y=0, "3-gram cloud")

wordcloud(FourGrams[,1], FourGrams[,2], min.freq =1, 
          random.order = F, ordered.colors = F, colors=palette)
text(x=0.5, y=0, "4-gram cloud")
par(mfrow = c(1, 1))



barplot(TwoGrams[1:20,2], cex.names=0.5, names.arg=TwoGrams[1:20,1], col="red", main="2-Grams", las=2)
barplot(ThreeGrams[1:20,2], cex.names=0.5, names.arg=ThreeGrams[1:20,1], col="green", main="3-Grams", las=2)
barplot(FourGrams[1:20,2], cex.names=0.5, names.arg=FourGrams[1:20,1], col="blue", main="4-Grams", las=2)
```

# Summary
For the final algorithm, I plan to design a ShinyApp that can help to predict by Markov chains n-gram model implementation

Steps 

* Finding and counting word pairs
* From word pairs to arbitrary n-grams
* N-gram model
* Generative text with Markov chains


