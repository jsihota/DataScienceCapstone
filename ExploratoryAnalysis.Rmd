---
title: "Exploratory analysis"
author: "jagdeep s sihota"
date: "July 24, 2015"
output: html_document
---
# Synnopsis
The goal of this report is to perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora and understand frequencies of words and word pairs. Around the world, people are spending an increasing amount of time on their mobile devices for email, social networking, banking and a whole range of other activities. But typing on mobile devices can be a serious pain. The keyboard presents three options for what the next word might be. For example, the three words might be gym, store, restaurant. We will work on understanding and building predictive text models like those used by SwiftKey. This report will start with the basics, analyzing a large corpus of text documents to discover the structure in the data and how words are put together. 
We will focues on 
  
# Dataset
The data is from a corpus called HC Corpora (www.corpora.heliohost.org). See the readme file at http://www.corpora.heliohost.org/aboutcorpus.html for details on the corpora available. The files have been language filtered but may still contain some foreign text. The dataset can be downloaded from https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip.

The dataset consists of data collected from blogs posts, twitter tweets, and online news articles. The dataset from these data sources are saved in .txt format as:

* en_US.blogs.txt
* en_US.twitter.txt
* en_US.news.txt

The data sources come in 4 different languages:

* US English (en_US)
* Finnish (fi_FI)
* Russian (ru_RU)
* German (de_DE)

# Load Data

```{r, echo=FALSE,chunk1,cache=TRUE}
setwd("~/git/DataScienceCapstone")
# download file from URL
if (!file.exists("Coursera-SwiftKey.zip")) {
  download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", destfile="Coursera-SwiftKey.zip",method = "curl")
}
# unzip file
if (!file.exists("final")) {
  unzip("Coursera-SwiftKey.zip")
}

setwd("~/git/DataScienceCapstone/final/en_US")
# Read files into variables
newsData <- readLines(file("en_US.news.txt"))
blogData <- readLines(file("en_US.blogs.txt"))
twitterData <- readLines(file("en_US.twitter.txt"))

library(plyr)
library(knitr)
## Create the list for summary
list_en<-list(newsData, blogData,twitterData)
names(list_en)<- c("en_US_news", "en_US_blogs","en_US_twitter")

## Define the function for Word-counting
WordCounting<-function(x) length(unlist(strsplit(x,split = " ")))
ldply(list_en,c("object.size","length","WordCounting"))->output
names(output) <- c("File Name","Size in bytes", "Line Counts", "Word Counts")
```

# Summarise the data
```{r kable, echo=FALSE, include = TRUE}
## Output the summary:Object size,Lines,Words
library(knitr)
kable(output)
```
## sampling data and data cleanup
For  performance reasons, only the first 10000 lines are loaded for the purpose of this report and cleanup the remove 

* Whitespace
* Numbers
* Punctuation
* stopwords

# Exploratory Data Analysis

## Word Frequency
A term-document matrix represents the relationship between terms and documents, where each row stands for a term and each column for a document, and an entry is the number of occurrences of the term in the document

```{r, echo=FALSE,chunk2,cache=TRUE}
library(tm)
merged <- paste(newsData[1:10000], blogData[1:10000], twitterData[1:10000])
corpus <- VCorpus(VectorSource(merged))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords())
```

```{r, echo=FALSE}
library(RWeka)
library(tm)
doc <- TermDocumentMatrix(corpus)
M1<-as.matrix(doc)
names <- rownames(M1)
freq <- rowSums(M1)
order<-order(freq,decreasing = T)
data1 <- data.frame(id = names,
                   news = M1[,1],
                   blogs = M1[,2],
                  twitter = M1[,3],
                  freq = freq)
data1 <- data1[order,] 

library(reshape2)
library(ggplot2)
melt(data1[1:20,],id="id")->plotdata
p <- ggplot(data=plotdata[1:60,],aes(id, value,fill=variable))
p <- p + geom_bar(stat="identity")+theme(axis.text.x=element_text(angle=45, hjust=1))
print(p)

library(wordcloud)
set.seed(123)
data1[1:100,]->plotwordcloud
pal <- brewer.pal(9,"BuGn")
    pal <- pal[-(1:4)]
wordcloud(plotwordcloud$id,freq=plotwordcloud$freq,random.order = F,color=pal)

```

## Find n-grams on the data
```{r, echo=FALSE,chunk3,cache=TRUE}
corpusDf <-data.frame(text=unlist(sapply(corpus, 
                  `[`, "content")), stringsAsFactors=F)

findNGrams <- function(corp, grams) {
  ngram <- NGramTokenizer(corp, Weka_control(min = grams, max = grams,
                      delimiters = " \\r\\n\\t.,;:\"()?!"))
  ngram2 <- data.frame(table(ngram))
  #pick only top 25
  ngram3 <- ngram2[order(ngram2$Freq,decreasing = TRUE),][1:100,]
  colnames(ngram3) <- c("String","Count")
  ngram3
}

TwoGrams <- findNGrams(corpusDf, 2)
ThreeGrams <- findNGrams(corpusDf, 3)
FourGrams <- findNGrams(corpusDf, 4)
```

```{r, echo=FALSE}
require(RColorBrewer)

par(mfrow = c(1, 3))
palette <- brewer.pal(8,"Dark2")

wordcloud(TwoGrams[,1], TwoGrams[,2], min.freq =1, 
          random.order = F, ordered.colors = F, colors=palette)
text(x=0.5, y=0, "2-gram cloud")

wordcloud(ThreeGrams[,1], ThreeGrams[,2], min.freq =1, 
          random.order = F, ordered.colors = F, colors=palette)
text(x=0.5, y=0, "3-gram cloud")

wordcloud(FourGrams[,1], FourGrams[,2], min.freq =1, 
          random.order = F, ordered.colors = F, colors=palette)
text(x=0.5, y=0, "4-gram cloud")
par(mfrow = c(1, 1))


barplot(TwoGrams[1:20,2], cex.names=0.5, names.arg=TwoGrams[1:20,1], col="red", main="2-Grams", las=2)
barplot(ThreeGrams[1:20,2], cex.names=0.5, names.arg=ThreeGrams[1:20,1], col="green", main="3-Grams", las=2)
barplot(FourGrams[1:20,2], cex.names=0.5, names.arg=FourGrams[1:20,1], col="blue", main="4-Grams", las=2)
```

# Summmary
For the final algorithm, I plan to design a ShinyApp that can help to predict by Markov chains n-gram model implementation

Steps 

* Finding and counting word pairs
* From word pairs to arbitrary n-grams
* N-gram model
* Generative text with Markov chains


