---
title: "Exploratory analysis"
author: "jagdeep s sihota"
date: "July 24, 2015"
output: html_document
---
=Synnopsis
The goal of this report is to perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora and understand frequencies of words and word pairs. Around the world, people are spending an increasing amount of time on their mobile devices for email, social networking, banking and a whole range of other activities. But typing on mobile devices can be a serious pain. The keyboard presents three options for what the next word might be. For example, the three words might be gym, store, restaurant. We will work on understanding and building predictive text models like those used by SwiftKey. This report will start with the basics, analyzing a large corpus of text documents to discover the structure in the data and how words are put together. 
We will focues on 
  
=Dataset
The data is from a corpus called HC Corpora (www.corpora.heliohost.org). See the readme file at http://www.corpora.heliohost.org/aboutcorpus.html for details on the corpora available. The files have been language filtered but may still contain some foreign text. The dataset can be downloaded from https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip.

The dataset consists of data collected from blogs posts, twitter tweets, and online news articles. The dataset from these data sources are saved in .txt format as:

en_US.blogs.txt
en_US.twitter.txt
en_US.news.txt
The data sources come in 4 different languages:

US English (en_US)
Finnish (fi_FI)
Russian (ru_RU)
German (de_DE)
== Load Data
```{r, recho=FALSE}
setwd("~/git/DataScienceCapstone")
# download file from URL
if (!file.exists("Coursera-SwiftKey.zip")) {
  download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", destfile="Coursera-SwiftKey.zip",method = "curl")
}
# unzip file
if (!file.exists("final")) {
  unzip("Coursera-SwiftKey.zip")
}

setwd("~/git/DataScienceCapstone/final/en_US")
# Read files into variables
newsData <- readLines(file("en_US.news.txt"))
blogData <- readLines(file("en_US.blogs.txt"))
twitterData <- readLines(file("en_US.twitter.txt"))

print(paste("News Data Length = ", length(newsData),
            ", News Blog Length = ", length(blogData),
            ", News twitter Length = ", length(twitterData)
            ))

lengths <- c(length(blogData),length(newsData),length(twitterData))
lengths <- data.frame(lengths)
lengths$names <- c("blogs","news","twitter")
ggplot(lengths,aes(x=names,y=lengths)) +
  geom_bar(stat='identity',fill='cornsilk',color='grey60') + 
  xlab('Source') + ylab('Total Lines') + coord_flip() + 
  geom_text(aes(label=format(lengths,big.mark=","),size=3),vjust=-0.2) +
  scale_y_continuous(limits=c(0,2600000)) +
  theme(legend.position='none') + 
  ggtitle('Total Line Count by Text Source')

```

== sampling data
For  performance reasons, only the first 10000 lines are loaded for the purpose of this milestone report.
```{r, echo=FALSE}
library(tm)
merged <- paste(newsData[1:10000], blogData[1:10000], twitterData[1:10000])
corpus <- VCorpus(VectorSource(merged))
```

== Data Clean up
Remove
  - Whitespace
  - Numbers
  - Punctuation
  - stopwords
```{r, echo=FALSE}
TrimSpace <- function( x ) {
  gsub("(^[[:space:]]+|[[:space:]]+$)", "", x)
}

TrimNonAlphanumeric<- function( x ) {
  gsub("[^[:alnum:] ]", "", x)
}

corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords())
library(RWeka)

corpusDf <-data.frame(text=unlist(sapply(corpus, 
                  `[`, "content")), stringsAsFactors=F)

findNGrams <- function(corp, grams) {
  ngram <- NGramTokenizer(corp, Weka_control(min = grams, max = grams,
                      delimiters = " \\r\\n\\t.,;:\"()?!"))
  ngram2 <- data.frame(table(ngram))
  #pick only top 25
  ngram3 <- ngram2[order(ngram2$Freq,decreasing = TRUE),][1:100,]
  colnames(ngram3) <- c("String","Count")
  ngram3
}

TwoGrams <- findNGrams(corpusDf, 2)
ThreeGrams <- findNGrams(corpusDf, 3)
FourGrams <- findNGrams(corpusDf, 4)
```


Exploratory Data Analysis

```{r, echo=FALSE}
par(mfrow = c(1, 1))

barplot(TwoGrams[1:20,2], cex.names=0.5, names.arg=TwoGrams[1:20,1], col="red", main="2-Grams", las=2)
barplot(ThreeGrams[1:20,2], cex.names=0.5, names.arg=ThreeGrams[1:20,1], col="green", main="3-Grams", las=2)
barplot(FourGrams[1:20,2], cex.names=0.5, names.arg=FourGrams[1:20,1], col="blue", main="4-Grams", las=2)
```

Creating a prediction algorithm and Shiny app

